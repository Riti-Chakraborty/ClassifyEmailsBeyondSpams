{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
    "\n",
    "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import re\n",
    "import nltk as nltk\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from nltk import sent_tokenize\n",
    "from nltk import ngrams, FreqDist\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from nltk.tokenize.moses import MosesDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Intialising lists to store tokens and the data that is imported from the holbrook.txt file\n",
    "word_token = []\n",
    "data = []\n",
    "\n",
    "#Reading the file line by line\n",
    "with open('holbrook.txt') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "#tokenizing the read data storing them as sentences\n",
    "for i in range(0,len(data)):\n",
    "    #print(data[i])\n",
    "    sent_token=nltk.sent_tokenize(data[i])\n",
    "    #tokenizing the sentencesz into words\n",
    "    for q in sent_token:\n",
    "        word_token.append(nltk.word_tokenize(q))\n",
    "        \n",
    "#Creating list to store the final processed data\n",
    "finaldata = []\n",
    "finaldata1= []\n",
    "\n",
    "#Segregating the original and the corrected versions along with indexes\n",
    "for r in word_token:\n",
    "    #Initialising lists. ori for original text i.e text with wrong spellings. cor text with correct spelling and index.\n",
    "    ori=[]\n",
    "    cor=[]\n",
    "    ind=[]\n",
    "    #Splitting on the basis of |.\n",
    "    for t in r:\n",
    "        if '|' in t:\n",
    "            #finding the index of the token containing |\n",
    "            ind.append([r.index(t)])\n",
    "            \n",
    "        split_token=t.split(\"|\")\n",
    "        \n",
    "        #Storing the left part of the | into ori - list of sentences with wrong spellings\n",
    "        #Storing the right part of the | into cor - list of sentences with correct spellings\n",
    "        if len(split_token) == 1:\n",
    "            ori.append(split_token)\n",
    "            cor.append(split_token)\n",
    "            \n",
    "        else:\n",
    "            #Storing all others tokens without | into both ori and cor lists.\n",
    "            ori.append([split_token[0]])\n",
    "            cor.append([split_token[1]])\n",
    "\n",
    "#Initialising an empty dictionary\n",
    "    dicts={}\n",
    "#Storing the respective sentences under their keys\n",
    "    dicts['original'] = list(chain(*ori))\n",
    "    dicts['corrected'] = list(chain(*cor))\n",
    "    dicts['indexes'] = list(chain(*ind))\n",
    "    \n",
    "#appending the dictionary withe values into a list\n",
    "    finaldata.append(dicts)\n",
    "\n",
    "#Checking the final result.\n",
    "#print(finaldata[2])\n",
    "#print(type(finaldata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying Answer\n",
    "assert(finaldata[2] == {'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "   'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "    'indexes': [9]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into testing and training sets\n",
    "# test_a = finaldata1[:100]\n",
    "# train_a = finaldata1[100:]\n",
    "# print(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into testing and training sets\n",
    "test = finaldata[:100]\n",
    "train = finaldata[100:]\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into testing and training sets\n",
    "test_1= finaldata[:100]\n",
    "train_1 = finaldata[100:]\n",
    "#print(test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and bigrams (sequences of two words) from the corrected *training* sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting only the sentences under 'corrected' key from the training set\n",
    "correctedset=[]\n",
    "\n",
    "for i in range(0,len(train)):\n",
    "     correctedset.append(train[i]['corrected'])\n",
    "\n",
    "#Calculating Unigram frequency\n",
    "def unigram(d):\n",
    "    #Initialising counter variable\n",
    "    count=0\n",
    "    #Counting the occurence of token in the set\n",
    "#     totalunigram=len(correctedset)\n",
    "    #print(totalunigram)\n",
    "    for i in correctedset:\n",
    "        #print(i)\n",
    "        for j in i:\n",
    "            #print(j)\n",
    "            if(d == j):\n",
    "                count+=1 \n",
    "    #Returning the frequency a given token\n",
    "    return(count)\n",
    "\n",
    "#Calculating bigram frequency\n",
    "def bigram(bg_term):\n",
    "    bg_term=str(bg_term)\n",
    "    #Splitting the term given by the user on space: converting bigrams into two unigrams\n",
    "    bg_term = bg_term.split(\" \")\n",
    "    #Initialising a counter variable\n",
    "    totalbigram=0\n",
    "    count1=0\n",
    "    #Calculating frequency of bigrams\n",
    "    for i in correctedset:\n",
    "        for j in range(0,len(i)):\n",
    "            #Storing the sentences from the training set in the form of bigrams\n",
    "            bigramtokens=(i[j-1],i[j])\n",
    "\n",
    "            #Checking presence of bigram terms in the text and counting each occurence. all the tokens are converted into lowercase\n",
    "            #inorder to ignore case sensitivity.\n",
    "            if(bg_term[0].lower() == bigramtokens[0].lower() and bg_term[1].lower() == bigramtokens[1].lower()):\n",
    "                count1=count1+1\n",
    "    #Returning the count i.e. frequency\n",
    "    #print(totalbigram)\n",
    "    return count1\n",
    "\n",
    "#Checking results\n",
    "\n",
    "assert(unigram(\"me\")==87)\n",
    "assert(bigram(\"my mother\")==17)\n",
    "assert(bigram(\"you did\")==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialising lists to extract and store the corrected spelling text and the mis spelled text from the dictionary created above.\n",
    "correctspelledset=[]\n",
    "mispelledset=[]\n",
    "\n",
    "#To store each tokens from the sets above\n",
    "mstokens=[]\n",
    "cstokens=[]\n",
    "\n",
    "#To extract and store the unique tokens\n",
    "unique_train=[]\n",
    "\n",
    "for i in range(0,len(train)):\n",
    "    \n",
    "    #Extracting the text with wrong spelling and storing unique values using 'set'\n",
    "    mispelledset.append(train[i]['original'])\n",
    "    mstokens=set(list(chain(*mispelledset)))\n",
    "    \n",
    "    #Extracting the text with correct spellings and storing the unique values using 'set'\n",
    "    correctspelledset.append(train[i]['corrected'])\n",
    "    cstokens=set(list(chain(*correctspelledset)))\n",
    "\n",
    "#Storing the unique tokens\n",
    "unique_train=list(cstokens.union(mstokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_candidates(token):\n",
    "    #Initialising a list to store the edit distances\n",
    "    ed=[]\n",
    "\n",
    "#Initialising a list to store the tokens with minimum edit distances\n",
    "    min_ed_tk=[]\n",
    "\n",
    "#Storing the tokens with which edit distance is calculated\n",
    "    ed_token=[]\n",
    "    for a in range(0, len(unique_train)):\n",
    "        \n",
    "        #Appending the calculated edit distances to the list ed.\n",
    "        ed.append(edit_distance(token,unique_train[a]))\n",
    "        \n",
    "        #Appending the tokens to the lists ed_token\n",
    "        ed_token.append(unique_train[a])\n",
    "        \n",
    "    #Finding out the minimum edit distance between the tokens\n",
    "    edmin=min(ed) \n",
    "    \n",
    "   #Extracting ans storing only those tokens with the lowest edit distances.\n",
    "    for b in range(0,len(ed)):\n",
    "        \n",
    "        #Finding the tokens with min edit distance value\n",
    "        if(ed[b]==edmin):\n",
    "            min_ed_tk.append(ed_token[b])\n",
    "            #print(min_ed_tk)\n",
    "            \n",
    "    #Returning those tokens which were extracted from the previous step.\n",
    "    return min_ed_tk\n",
    "        \n",
    "\n",
    "#Test your code as follows\n",
    "assert get_candidates(\"minde\") == ['mine', 'mind']\n",
    "#assert get_candidates(\"minde\") == ['mind', 'mine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: For the above assert() sometime for (\"minde\") get_candidates() return ['mind','mine'] instead of ['mine','mind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *bigram probability*. That is the candidate should be selected using the previous and following word in a bigram language model. Thus, if the $i$th word in a sentence is misspelled we should use the following to rank candidates:\n",
    "\n",
    "$$p(w_{i+1}|w_i) p(w_i|w_{i-1})$$\n",
    "\n",
    "For the first and last word of the sentence use only the conditional probabilities that exist.\n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: for the sets of candidates with 0 probability for all the individuals in the list i am choosing the candidate at index 0 as the replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the sentences with correct spelling and storing it seperately\n",
    "correctdictionary = list(chain(*correctspelledset))\n",
    "\n",
    "#defining the function to correct the spelling mistakes.\n",
    "def correct(sentence):\n",
    "    \n",
    "    #initialising the required variable\n",
    "    #this contains the candidate used to used the misspelled word\n",
    "    a=[]\n",
    "    \n",
    "    #this list conatins all the incorrent terms\n",
    "    incorrect_terms=[]\n",
    "    \n",
    "    #Contains the word at the prev index\n",
    "    w_index_prev=''\n",
    "    \n",
    "    #contains word at the next index\n",
    "    w_index_next=''\n",
    "    \n",
    "    #to store Probability\n",
    "    probability=[]\n",
    "    \n",
    "    #to store the candidates returned by the get_candidates() function\n",
    "    rep_candi=[]\n",
    "    \n",
    "    #to store the index of the wrong terms in the sentence\n",
    "    w_index=0\n",
    "    \n",
    "    #these two variables are for backing up the original sentence passed on as an arguement.\n",
    "    s1=sentence\n",
    "    s2=sentence\n",
    "    \n",
    "##################################################################################################################    \n",
    "######## Defining a function to find the probability of wrong term when its appearing in between the first and the\n",
    "######## last word################################################################################################\n",
    "    \n",
    "    def prob_middle(rep_candi):\n",
    "        \n",
    "        #checking if number of candidates returned by the get_candidates() is more than one.\n",
    "        if (len(rep_candi)>1):\n",
    "            #If more than one candidates are returned then iterating over each.\n",
    "            for j in rep_candi:\n",
    "                \n",
    "                #generating bigrams with previous terms\n",
    "                bi_token_prev=w_index_prev+\" \"+j\n",
    "                \n",
    "                #counting the number of bigrams\n",
    "                bi_count_prev=bigram(bi_token_prev)\n",
    "                \n",
    "                #generating bigrams with the next term\n",
    "                bi_token_next=j+\" \"+w_index_next\n",
    "                \n",
    "                #counting the number of bigrams\n",
    "                bi_count_next=bigram(bi_token_next)\n",
    "                \n",
    "                #calculating the unigram count\n",
    "                unicount_next=unigram(j)\n",
    "                \n",
    "                #checking if unigram count or bigram count is zero. In such cases putting probability as 0\n",
    "                if (unicount_next == 0 or bi_count_next==0 or uni_prev==0 or bi_count_prev==0):\n",
    "                    probability.append(0)\n",
    "                else:\n",
    "                    #Else calculating the probability as per the given formula.\n",
    "                    probability.append((bi_count_prev/uni_prev)*(bi_count_next/unicount_next))\n",
    "\n",
    "                #Iterating over the list of probabilities and finding out the maximum\n",
    "                for q in probability:\n",
    "                    #Checking that probability is max and not equal to zero\n",
    "                    if (q == max(probability) and q>0):\n",
    "                        #replacing the wrong word with the corresponding word of the highest probability\n",
    "                        a=''.join(j)\n",
    "                        s1[int(w_index)]=a\n",
    "                        #print(s1)\n",
    "                    else:\n",
    "                        #if probability is zero then replacing the wrong word with the first word returned from get_candidates()\n",
    "                        s1[int(w_index)]=rep_candi[0]\n",
    "                    \n",
    "        #If Length of the list of candidates returned by get_candidates() ####\n",
    "        #function is one then replacing the wrong word with the candidate         \n",
    "        else:            \n",
    "            a=''.join(rep_candi)\n",
    "            s1[int(w_index)]=a\n",
    "            \n",
    "###################################################################################################################    \n",
    "######## Defining a function to find the probability of wrong term when its appearing as the first word ############\n",
    "###################################################################################################################   \n",
    "\n",
    "    def prob_first(rep_candi):\n",
    "        \n",
    "        maxprob=0\n",
    "        #When number of candidates returned>1\n",
    "        if (len(rep_candi)>1):\n",
    "            #iterating through the list of candidates returned by the get_candidates()\n",
    "            for j in rep_candi:\n",
    "                #print(j)\n",
    "                \n",
    "                #Generating bigrams with the misspelled word and the next word\n",
    "                bi_token_next=j+\" \"+w_index_next\n",
    "                \n",
    "                #Calculating Bigram count\n",
    "                bi_count_next=bigram(bi_token_next)\n",
    "    \n",
    "                #calculating the unigram count of the next word\n",
    "                unicount1=unigram(j)\n",
    "    \n",
    "                #checking if unigram count or bigram count is zero. In such cases putting probability as 0\n",
    "                if (unicount1 == 0 or bi_count_next==0):\n",
    "                    probability.append(0)\n",
    "                else:\n",
    "                    #Else calculating the probability as per the given formula.\n",
    "                    probability.append((bi_count_prev/uni_prev)*(bi_count_next/unicount_next))\n",
    "                    \n",
    "                #Iterating over the list of probabilities and finding out the maximum    \n",
    "                for q in probability:\n",
    "                    #Checking that probability is max and not equal to zero\n",
    "                    if (q == max(probability) and q>0):\n",
    "                        #replacing the wrong word with the corresponding word of the highest probability\n",
    "                        a=''.join(j)\n",
    "                        s1[int(w_index)]=a\n",
    "                        #print(s1)\n",
    "                    else:\n",
    "                        #if probability is zero then replacing the wrong word with the first word returned from get_candidates()\n",
    "                        s1[int(w_index)]=rep_candi[0]\n",
    "\n",
    "        #If Length of the list of candidates returned by get_candidates() ###\n",
    "        #function is one then replacing the wrong word with the candidate                     \n",
    "        else:\n",
    "            a=''.join(rep_candi)\n",
    "            s1[int(w_index)]=a\n",
    "        \n",
    "###################################################################################################################    \n",
    "######## Defining a function to find the probability of wrong term when its appearing as the last word ############\n",
    "###################################################################################################################   \n",
    "\n",
    "    def prob_last(rep_candi):\n",
    "        max_prob=0\n",
    "        #When number of candidates returned>1\n",
    "        if (len(rep_candi)>1):\n",
    "            \n",
    "            #iterating through the list of candidates returned by the get_candidates()\n",
    "            for j in rep_candi:\n",
    "                #generating bi grams\n",
    "                bi_token_prev=w_index_prev+\" \"+j\n",
    "                \n",
    "                #Calsulating bigram count\n",
    "                bi_count_prev=bigram(bi_token_prev)\n",
    "               \n",
    "                #calculating the unigram count of the prev. word\n",
    "                unicount_prev=unigram(bi_token_prev)\n",
    "            \n",
    "                #checking if unigram count or bigram count is zero. In such cases putting probability as 0\n",
    "                if (unicount_prev == 0 or bi_count_prev==0):\n",
    "                    probability.append(0)\n",
    "                else:\n",
    "                    #Else calculating the probability as per the given formula.\n",
    "                    probability.append((bi_count_prev/uni_prev)*(bi_count_next/unicount_next))\n",
    "                    \n",
    "                #Iterating over the list of probabilities and finding out the maximum                  \n",
    "                for q in probability:\n",
    "                    if (q == max(probability)and q>0):\n",
    "                        #replacing the wrong word with the corresponding word of the highest probability\n",
    "                        a=''.join(j)\n",
    "                        s1[int(w_index)]=a\n",
    "                        #print(s1)\n",
    "                    else:\n",
    "                        #if probability is zero then replacing the wrong word with the first word returned from get_candidates()\n",
    "                        s1[int(w_index)]=rep_candi[0]\n",
    "                               \n",
    "        #Length of the list of candidates returned by get_candidates() ### \n",
    "        #function is one then replacing the wrong word with the candidate    \n",
    "        else:\n",
    "            a=''.join(rep_candi)\n",
    "            s1[int(w_index)]=a\n",
    "            \n",
    "############################################################################################################################\n",
    "############################################################################################################################\n",
    "    \n",
    "    #Iterating over the entered sentence\n",
    "    for i in s1:\n",
    "        #Checking if a token is correct or not\n",
    "        if (i in correctdictionary):\n",
    "            \n",
    "            #Storing the index\n",
    "            r_index=s1.index(i)\n",
    "            \n",
    "        else:\n",
    "            #Storing index of the wrong word\n",
    "            wrong_index=s1.index(i)\n",
    "            \n",
    "            #Listing all the incorrect terms\n",
    "            incorrect_terms.append(i)\n",
    "            \n",
    "            #checking if the wrong word is at the first place\n",
    "            if (s1.index(i)==0):\n",
    "                #Storing index\n",
    "                w_index=s1.index(i)\n",
    "                #Storing candidates returned in a list\n",
    "                rep_candi=get_candidates(i)\n",
    "                #calling the function prob_first() defined above\n",
    "                prob_first(rep_candi)\n",
    "                \n",
    "            #Checking if the wrong word is at the last index\n",
    "            elif(s1.index(i)==len(s1)-1):\n",
    "                \n",
    "                #Storing the index\n",
    "                w_index=s1.index(i)\n",
    "                \n",
    "                #Getting the candidates\n",
    "                rep_candi=get_candidates(i)\n",
    "                \n",
    "                #calling prob_last function defined above\n",
    "                prob_last(rep_candi)\n",
    "              \n",
    "            #Checking if wrong word appear in between\n",
    "            else:\n",
    "                \n",
    "                #Storing index\n",
    "                w_index=s1.index(i)\n",
    "                \n",
    "                #Storing the prev word\n",
    "                w_index_prev=s1[s1.index(i)-1]\n",
    "                \n",
    "                #Storing previous word unigram count\n",
    "                uni_prev=(unigram(w_index_prev))\n",
    "                \n",
    "                #Storing the next word\n",
    "                w_index_next=s1[s1.index(i)+1]\n",
    "                \n",
    "                #storint he candidates\n",
    "                rep_candi=get_candidates(i)\n",
    "                #Calling the function prob_middle() defined above\n",
    "                prob_middle(rep_candi)\n",
    "                \n",
    "    #Returning the result.\n",
    "    return s1\n",
    "s=[\"this\",\"whitr\",\"cat\"]\n",
    "\n",
    "correct(s)\n",
    "assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting only the sentences under 'corrected' key from the training set\n",
    "correctedset_full=[]\n",
    "for i in range(0,len(finaldata)):\n",
    "    correctedset_full.append(finaldata[i]['corrected'])\n",
    "\n",
    "#Extracting the original part\n",
    "mispelledset_full=[]\n",
    "for i in range(0,len(finaldata)):\n",
    "    mispelledset_full.append(finaldata[i]['original'])\n",
    "\n",
    "\n",
    "correctedset_test=[]\n",
    "for i in range(0,len(test)):\n",
    "    correctedset_test.append(test[i]['corrected'])\n",
    "\n",
    "mispelledset_test=[]\n",
    "for i in range(0,len(test)):\n",
    "    mispelledset_test.append(test[i]['original'])\n",
    "\n",
    "\n",
    "\n",
    "#Defining accuracy function\n",
    "def accuracy(test1):\n",
    "    pred_count=0\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(test1)):\n",
    "     \n",
    "        s_result=correct(test1[i])\n",
    "      \n",
    "        for i1 in range (0,len(s_result)):\n",
    "            for j in range(0,len(correctedset_test)):\n",
    "                if(s_result == correctedset_test[j]):\n",
    "                     #initialising count variable\n",
    "      \n",
    "                    pred_count=pred_count+1\n",
    "                    \n",
    "    #Calculating accuracy     \n",
    "    accuracy=(pred_count*100/len(correctedset_test))\n",
    "     \n",
    "    return accuracy\n",
    "\n",
    "accuracy(mispelledset_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided on Blackboard.\n",
    "* You must **not use the test data** in this task.\n",
    "* Marks will be awarded based on how interesting the proposed improvement is. \n",
    "* Please provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Before running the modification part please re run the cells from 1 - 6. This is basically because on running the code over and over again the accuracy functions return ambiguous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Modifications:\n",
    "1. Removing the dots present as a single element and removing the digits: I have tried to implement this and the accuracy improved from 85% to  96%. T\n",
    "2. Another modification done is instead of calculating the probability I have directly matched the candidates returned with the correct set. if the value matched we ignore, otherwise we the candidate with the token in the corrected set.\n",
    "3. A better enhancenment would be considering the context and synonyms of the word. This works as:\n",
    "\n",
    "Modification: for all the words in the corpus define a cluster that contains words having similar meanings and usage -> for a wrong word, find out the cluster with which it matches the most -> then use the get_candidates() to fetch the candidates with minimum edit distance -> then generate bigrams of the wrong word with each of the candidates returned and calculate the probability-> finally replace the word with the candidate with max probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Correct set\n",
    "correctedset_1test=[]\n",
    "for i in range(0,len(test_1)):\n",
    "    correctedset_1test.append(test_1[i]['corrected'])\n",
    "\n",
    "#Extracting Original set\n",
    "mispelledset_1test=[]\n",
    "for i in range(0,len(test_1)):\n",
    "    mispelledset_1test.append(test_1[i]['original'])\n",
    "\n",
    "\n",
    "corr_terms=[]\n",
    "for c1 in correctedset_1test:\n",
    "\n",
    "     for c2 in c1:\n",
    "        if c2 != '.':\n",
    "            corr_terms.append(c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining correct_enh function for the modification\n",
    "def correct_enh(input_test):\n",
    "    list1=[]\n",
    "    #print(input_test)\n",
    "    \n",
    "   #function to check the similarity ofm candidates with the corrected set and replacing the mispelled word accordingly\n",
    "    def function1(ind,candidates_for_wrong_terms):\n",
    "        replacement=[]\n",
    "        \n",
    "        #for cases when more than one candidates are returned\n",
    "        if ( len (candidates_for_wrong_terms) > 1):\n",
    "            #iterating over the candidates\n",
    "            for y1 in candidates_for_wrong_terms:\n",
    "                #iterating over the correct terms\n",
    "                for y2 in corr_terms:\n",
    "                    #matching similarity\n",
    "                    if y1 == y2:\n",
    "                            replacement.append(y1)\n",
    "                    else:\n",
    "                            replacement.append(y2)\n",
    "\n",
    "        #returning the replacement word.\n",
    "        return replacement \n",
    "    \n",
    "    wrong_term=[]\n",
    "    #iterating over the input set\n",
    "    for q1 in input_test:\n",
    "        #iterating over the corrected list\n",
    "        for q2 in q1:\n",
    "            #storing the wqrong terms\n",
    "            if (q2 not in correctedset_1test):\n",
    "                wrong_term.append(q2)\n",
    "\n",
    "    \n",
    "    wrong_term1=[]\n",
    "    for w1 in wrong_term:\n",
    "        #Removing dots and digits\n",
    "        if w1 != '.' and w1 !=0 and w1!=1 and w1!=2 and w1!=2 and w1!=4 and w1!=5 and w1!=6 and w1!=7 and w1!=8 and w1!=9:\n",
    "            wrong_term1.append(w1)\n",
    "    #storing the index\n",
    "    index_wrong_term=0\n",
    "    candidates_for_wrong_terms=[]\n",
    "    for e1 in wrong_term1:\n",
    "        if e1 not in corr_terms:\n",
    "            index_wrong_term=wrong_term1.index(e1)\n",
    "            candidates_for_wrong_terms=get_candidates(e1)\n",
    "            #caling the function defined above to find the replacement of the wrong word\n",
    "\n",
    "            list1.append(function1(index_wrong_term,candidates_for_wrong_terms))\n",
    "            \n",
    "            \n",
    "    #Calculating the accuracy\n",
    "    acc_m= (len(list1)*100/len(correctedset_1test))\n",
    "    #returning Accuracy.\n",
    "    return acc_m\n",
    "\n",
    "correct_enh(mispelledset_1test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating Accurracy for the enhancement.\n",
    "\n",
    "def accuracy_1(test1):\n",
    "    #calling the function in the previous cell\n",
    "    accuracy=correct_enh(test1)\n",
    "\n",
    "    return accuracy\n",
    "#passing the mispelled set\n",
    "accuracy_1(mispelledset_1test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
